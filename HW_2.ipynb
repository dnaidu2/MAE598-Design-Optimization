{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88892302",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "#Problem Statement : Show that the stationary point (zero gradient) of the function \n",
    "$f = 2x_{1}^2 - 4x_{1}x_{2}+1.5x_{2}^2+x_{2}$\n",
    "is a saddle (with indefinite Hessian).\n",
    "- Find the directions of downslope away from the saddle.Find directions that reduce f.\n",
    "## Solution\n",
    "\n",
    "Given function <br>\n",
    "$f = 2x_{1}^2 - 4x_{1}x_{2}+1.5x_{2}^2+x_{2}$ <br>\n",
    "<br>\n",
    "#Gradient\n",
    "<br>\n",
    "$\\mathbf{g} = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_{1}} \\\\ \\frac{\\partial f}{\\partial x_{2}}\\end{bmatrix}\n",
    "= \\begin{bmatrix}4x_{1}-4x{2}\\\\-4x_{1}+3x_{2}+1\\end{bmatrix}$\n",
    "<br>\n",
    "#Zero gradient \n",
    "<br>\n",
    "$\\mathbf{g_{0}} = \\begin{bmatrix}0\\\\0\\end{bmatrix}$ <br>\n",
    "Function f has Stationary point/zero gradient at (1,1) <br>\n",
    "$\\mathbf{g} = \\begin{bmatrix}1\\\\1\\end{bmatrix}$ <br>\n",
    "To show that stationary point (1,1) is saddle point, we need to calculate hessian of the function. <br>\n",
    "#Hessian \n",
    "<br>\n",
    "$\\mathbf{H}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial x_{1}^2} & \\frac{\\partial^2 f}{\\partial x_{1}x_{2}}\\\\\n",
    "\\frac{\\partial^2f}{\\partial x_{1}x_{2}} & \\frac{\\partial^f}{\\partial x_{2}^2}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix} 4 & -4\\\\ -4 & 3 \\end{bmatrix}\n",
    "$\n",
    "### To show that stationary point of the function f is saddle point \n",
    "<br>\n",
    "Calculating eigen values <br>\n",
    "Let determinent of H be D <br>\n",
    "$ D = 4*3-4*4 = -4 $ <br>\n",
    "We know that Determinent of matrix is product of eigen values <br>\n",
    "Since the product of eigen values is negative, that shows that one eigen value is positive and another is negative. So, the hessian is indefinite and as well as function is indefinite. <br>\n",
    "So, the stationary point (1,1) is saddle point. Because the curve of the function is in the shape of a saddle. <br>\n",
    "$f(x_{0}) = 2(1)-4(1)(1)+1.5(1)+(1)^2 = 2-4+1.5+1 = 0.5$\n",
    "<br>\n",
    "#### Direction of Downslope that reduces f \n",
    "<br>\n",
    "Using Taylor's expansion <br>\n",
    "$f(x_{1},x_{2}) = f(x_{0})+ g_{0} \\begin{bmatrix} x_{1}-1 & x_{2}-1 \\end{bmatrix} + 0.5 \\begin{bmatrix} x_{1}-1 & x_{2}-1 \\end{bmatrix} \\begin{bmatrix}  4 & -4\\\\ -4 & 3 \\end{bmatrix} \\begin{bmatrix} x_{1}-1 \\\\ x_{2}-1 \\end{bmatrix}$\n",
    "<br>\n",
    "$f(x_{1},x_{2}) - f(x_{0}) = 0.5 \\begin{bmatrix} x_{1}-1 & x_{2}-1 \\end{bmatrix} \\begin{bmatrix}  4 & -4\\\\ -4 & 3 \\end{bmatrix} \\begin{bmatrix} x_{1}-1 \\\\ x_{2}-1 \\end{bmatrix}$\n",
    "<br>\n",
    "(Since gradient is zero) <br>\n",
    "$0.5 \\begin{bmatrix} x_{1}-1 & x_{2}-1 \\end{bmatrix} \\begin{bmatrix}  4 & -4\\\\ -4 & 3 \\end{bmatrix} \\begin{bmatrix} x_{1}-1 \\\\ x_{2}-1 \\end{bmatrix} \n",
    "= 0.5 \\begin{bmatrix} 4x_{1}- 4x_{2} & -4x_{1}+ 3x_{2}+1 \\end{bmatrix} \\begin{bmatrix} x_{1}-1 \\\\ x_{2}-1 \\end{bmatrix} \n",
    "= 0.5 [(4x_{1}- 4x_{2})(x_{1}-1)+ (-4x_{1}+ 3x_{2}+1) (x_{2}-1)]\n",
    "= 0.5 [(2x_{1}-x_{2})(2x_{1}-3x_{2})]$ <br>\n",
    "Downslope reduces f, when $f(x_{1},x_{2}) - f(x_{0}) < 0$ <br>\n",
    "$\\Longrightarrow 0.5 [(2x_{1}-x_{2})(2x_{1}-3x_{2})] < 0$ <br>\n",
    "$\\Longrightarrow (2x_{1}-x_{2})(2x_{1}-3x_{2}) < 0$ \n",
    "<br>\n",
    "The only values that satisfies this condition are <br>\n",
    "$(2x_{1}-x_{2}) < 0 $ and $(2x_{1}-3x_{2}) > 0$\n",
    "<br>\n",
    "OR $(2x_{1}-x_{2}) > 0$ and $(2x_{1}-3x_{2}) < 0$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe8f0c",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "## a) Find the nearest point in the plane and check if the given problem is convex.\n",
    "<br>\n",
    "<img src = \"MAE 598_HW2-P2a-page1.jpg\" alt = \"MAE 598_HW2-P2a-page-001\" title = \"Nearest point page 1\" />\n",
    "<br>\n",
    "<img src = \"MAE 598_HW2_P2a.jpg\" alt = \"MAE 598_HW2-P2a-page-002\" title = \"Nearest point page 2\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa670ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equation 1:\n",
      "Eq(10*y + 12*z, 8)\n",
      "Equation 2\n",
      "Eq(12*y + 20*z, 14)\n",
      "Equation 3\n",
      "Eq(x + 2*y + 3*z, 1)\n",
      "The point in the plane nearest to (-1,0,1)^T is:\n",
      "{x: -15/14, y: -1/7, z: 11/14}\n"
     ]
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "x,y, z = symbols('x,y,z')\n",
    "eq1 = Eq((10*y+12*z), 8)\n",
    "print(\"Equation 1:\")\n",
    "print(eq1)\n",
    "eq2 = Eq((12*y+20*z), 14)\n",
    "print(\"Equation 2\")\n",
    "print(eq2)\n",
    "eq3 = Eq((x+2*y+3*z), 1)\n",
    "print(\"Equation 3\")\n",
    "print(eq3)\n",
    "print(\"The point in the plane nearest to (-1,0,1)^T is:\")\n",
    "  \n",
    "print(solve((eq1, eq2,eq3), (x, y, z)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849d744f",
   "metadata": {},
   "source": [
    "#Gradient\n",
    "<br>\n",
    "$\\mathbf{g} = \\begin{bmatrix}\\frac{\\partial f}{\\partial y} \\\\ \\frac{\\partial f}{\\partial z}\\end{bmatrix}\n",
    "= \\begin{bmatrix}10y+12z-8 \\\\ 12y+20z-14 \\end{bmatrix}$\n",
    "<br>\n",
    "#Hessian \n",
    "<br>\n",
    "$\\mathbf{H}=\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial^2 f}{\\partial y^2} & \\frac{\\partial^2 f}{\\partial yz}\\\\\n",
    "\\frac{\\partial f}{\\partial yz} & \\frac{\\partial f}{\\partial z^2}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix} 10 & 12 \\\\ 12 & 20 \\end{bmatrix}$\n",
    "<br>\n",
    "Hessian is positive definite everywhere. Therefore, the given problem is __CONVEX__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b68fb1",
   "metadata": {},
   "source": [
    "## b) Gradient descent and Newton's algorithm\n",
    "### Gradient descent algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e66c8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.25,\n",
       " 0.4375,\n",
       " 0.578125,\n",
       " 0.68359375,\n",
       " 0.7626953125,\n",
       " 0.822021484375,\n",
       " 0.86651611328125,\n",
       " 0.8998870849609375,\n",
       " 0.9249153137207031,\n",
       " 0.9436864852905273,\n",
       " 0.9577648639678955,\n",
       " 0.9683236479759216,\n",
       " 0.9762427359819412,\n",
       " 0.9821820519864559,\n",
       " 0.9866365389898419,\n",
       " 0.9899774042423815,\n",
       " 0.9924830531817861,\n",
       " 0.9943622898863396,\n",
       " 0.9957717174147547,\n",
       " 0.996828788061066,\n",
       " 0.9976215910457995,\n",
       " 0.9982161932843496,\n",
       " 0.9986621449632622,\n",
       " 0.9989966087224467,\n",
       " 0.999247456541835,\n",
       " 0.9994355924063762,\n",
       " 0.9995766943047821]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = lambda x: (x - 1)**2  \n",
    "grad = lambda x: 2*(x - 1) \n",
    "eps = 1e-3  # termination criterion\n",
    "x0 = 0.  # initial guess\n",
    "k = 0  # counter\n",
    "soln = [x0]  # use an array to store the search steps\n",
    "x = soln[k]  # start with the initial guess\n",
    "error = abs(grad(x))  # compute the error. Note you will need to compute the norm for 2D grads, rather than the absolute value\n",
    "# a = 0.01  # set a fixed step size to start with\n",
    "\n",
    "# Armijo line search\n",
    "def line_search(x):\n",
    "    a = 1.  # initialize step size\n",
    "    phi = lambda a, x: obj(x) - a*0.8*grad(x)**2  # define phi as a search criterion\n",
    "    while phi(a,x)<obj(x-a*grad(x)):  # if f(x+a*d)>phi(a) then backtrack. d is the search direction\n",
    "        a = 0.5*a\n",
    "    return a\n",
    "\n",
    "while error >= eps:  # keep searching while gradient norm is larger than eps\n",
    "    a = line_search(x)\n",
    "    x = x - a*grad(x)\n",
    "    soln.append(x)\n",
    "    error = abs(grad(x))\n",
    "    \n",
    "soln  # print the search trajectory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
